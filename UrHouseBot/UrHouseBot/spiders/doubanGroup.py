# -*- coding: utf-8 -*-
import scrapy
import UrHouseBot
from UrHouseBot import Constants, parse_tool
import re
from UrHouseBot.items import UrhousebotItem
import datetime
import urllib
from PIL import Image
from scrapy import responsetypes
import random
import string
from private import doubanAccount, doubanPwd
import UrHouseBot.DouBanSelector as Selector

import json

# import requests
from UrHouseBot.NotifyTools import colorTitle, slackMe
from UrHouseBot.Constants import URLS
from UrHouseBot.DouBanSelector import SELECTOR

duetime = 7

doubanID = doubanAccount
doubanPWD = doubanPwd

redirectCount = {}


class DoubangroupSpider(scrapy.Spider):
    name = "doubanGroup"
    start = True
    authors = []
    groups = {}
    titles = {}
    configs = {}
    blockLink = []
    monitors = {}
    headers = Constants.headers

    custom_settings = {
        # "RANDOM_DELAY": 4,
        "DOWNLOADER_MIDDLEWARES": {
            "UrHouseBot.middlewares.RandomDelayMiddleware": 999,
            "UrHouseBot.middlewares.RandomUserAgentMiddlware": 200,
            "UrHouseBot.middlewares.ProxyMiddleware": 125,
            "UrHouseBot.middlewares.Redirect302Middleware": 1000,
        }
    }

    def infoLog(self, content):
        self.logger.info(DebugLevel.debug, content)

    def start_requests(self):
        if not doubanID:
            self.logger.info("æ²¡æœ‰è±†ç“£ id å’Œå¯†ç , é‡åˆ°éœ€è¦ç™»å½•çš„æƒ…å†µä¼šå¤±è´¥")

        self.configs["stop"] = False
        toCrawl = self.startCrawl()
        for request in toCrawl:
            yield request

    def needLogin(self, response):
        self.logger.info("çœ‹çœ‹è¦ä¸è¦ç™»å½•")
        if len(re.findall("ç™»å½•", response.xpath("string(.)").get().strip())) > 0:
            self.logger.info(" éœ€è¦ç™»å½•")
            yield scrapy.FormRequest(
                URLS.login,
                headers=self.headers,
                meta={"cookiejar": 1},
                callback=self.parse_before_login,
            )
        else:
            self.logger.info("ä¸éœ€è¦ç™»å½•")
            yield self.startCrawl()

    def parse_before_login(self, response):
        """
        ç™»å½•è¡¨å•å¡«å……ï¼ŒæŸ¥çœ‹éªŒè¯ç 
        """
        if not doubanID:
            return

        self.logger.info("ç™»å½•å‰è¡¨å•å¡«å……")
        captcha_id = response.xpath(SELECTOR.usernameField).extract_first()
        captcha_image_url = response.xpath(SELECTOR.userpwdField).extract_first()
        if captcha_image_url is None:
            self.logger.info("ç™»å½•æ—¶æ— éªŒè¯ç ")
            formdata = {
                "ck": "",
                "name": doubanAccount,  # '!!warning__è¿™é‡Œ å¡«ä¸Šä½ è±†ç“£çš„ ID__!!!',
                "password": doubanPwd,  # '!!warning__è¿™é‡Œå¡«ä¸Šè±†ç“£å¯†ç __!!',
                "remember": "false",
                "ticket": "",
            }
        else:
            self.logger.info("ç™»å½•æ—¶æœ‰éªŒè¯ç ")
            save_image_path = "./captcha.jpeg"
            # å°†å›¾ç‰‡éªŒè¯ç ä¸‹è½½åˆ°æœ¬åœ°
            urllib.urlretrieve(captcha_image_url, save_image_path)
            # æ‰“å¼€å›¾ç‰‡ï¼Œä»¥ä¾¿æˆ‘ä»¬è¯†åˆ«å›¾ä¸­éªŒè¯ç 
            open()
            try:
                im = Image.open("captcha.jpeg")
                im.show()
            except:
                pass
            # æ‰‹åŠ¨è¾“å…¥éªŒè¯ç 
            captcha_solution = input("æ ¹æ®æ‰“å¼€çš„å›¾ç‰‡è¾“å…¥éªŒè¯ç :")
            formdata = {
                "ck": "",
                "name": doubanAccount,
                "password": doubanPwd,
                "remember": "false",
                "ticket": "",
            }

        self.logger.info("ç™»å½•ä¸­")
        # æäº¤è¡¨å•
        return scrapy.FormRequest(
            URLS.submitLogin,
            headers=self.headers,
            formdata=formdata,
            callback=self.parse_after_login,
        )

    def parse_after_login(self, response):
        """
        éªŒè¯ç™»å½•æ˜¯å¦æˆåŠŸ
        """
        info = json.loads(response.body)
        self.logger.info(f"ç™»å½•ç»“æœ: {info}")
        if response.status != 200:
            self.logger.info("ç™»å½•å¤±è´¥")
            return

        if info["status"] == "failed":
            self.logger.info(f"ç™»å½•å¤±è´¥, {info['éœ€è¦å›¾å½¢éªŒè¯ç ']}")
        self.logger.info(f"ç™»å½•æˆåŠŸ")
        toCrawl = self.startCrawl()
        for request in toCrawl:
            yield request

    def startCrawl(self):
        self.logger.info("å¼€å§‹æŠ“å–")
        slackMe(
            "\n\n"
            + datetime.datetime.today().strftime("%Y-%m-%d %H:%M")
            + "=" * 50
            + datetime.datetime.today().strftime("%Y-%m-%d %H:%M")
            + "\n\n"
        )
        self.loadCrawled()
        url = URLS.groupsPage
        needCrawl = []
        for title, dic in self.titles.items():
            if isinstance(dic, str):
                continue
            if dic.get("hadReq") and not dic.get("hadResp"):
                link = dic.get("link")
                oTitle = dic.get("title")
                if self.isBlock(link, oTitle):
                    continue
                req = scrapy.Request(
                    link,
                    headers=self.headers,
                    dont_filter=True,
                    callback=self.parse_detail_article,
                )
                req.meta["tag_info"] = f'ä¸Šæ¬¡æœªå®Œæˆ: å¸–å­: -> {oTitle}:{dic.get("link")}'
                req.meta["title"] = oTitle
                needCrawl.append(req)

        return [
            scrapy.Request(
                url, dont_filter=True, headers=self.headers, callback=self.parse
            )
        ] + needCrawl

    def close(self, reason):
        msg = "ç»“æŸ" + reason
        time = datetime.datetime.today().strftime("%Y-%m-%d %H:%M")
        if reason == "finish" and "gidPage" in self.configs:
            del self.configs["gidPage"]
            msg = "å®Œæˆä¸€è½®" + time + "*" * 50

        self.saveCrawled()
        slackMe(msg)
        self.logger.info("å®Œæˆä¸€è½®")

    def parseIfNeed(self, response):
        # ç»„çš„å¸–å­å¤ªå°‘, æˆ–è€…æ—¥æœŸå¤ªæ—§, éƒ½ä¸è¿›è¡Œçˆ¬å–

        #  å°ç»„ id
        gid = response.meta.get("gid")
        t = response.meta.get("title")
        l = response.meta.get("link")

        dates = response.xpath('//td[@class="time"]/text()').getall()
        titles = response.xpath('//td[@class="title"]/a/text()').getall()
        links = response.xpath('//td[@class="title"]/a/@href').getall()
        validLinks = []
        if not dates:
            dates = []

        for index, d in enumerate(dates):
            if parse_tool.is_need_parse(d):
                validLinks.append(links[index])

        if len(validLinks) < 15:
            # å°†è¿™ä¸ªç»„æ ‡è®°ä¸ºä¸çˆ¬å–
            self.blockLink.append(l)
            self.groups[gid]["dead"] = True
            self.logger.info(
                f"ğŸš¯ğŸš¯è¯¥å°ç»„å·²é•¿æœŸä¸æ´»è·ƒ, åŠ è¿› dead å’Œ block: {t}, é¦–é¡µå¸–å­: {len(dates)} ä¸ª, url: {response.url}"
            )
            self.saveCrawled()

            # å°†é¡µé¢å†æ—¶é—´ä¸Šç¬¦åˆçš„, åˆ†æ
            for index, link in enumerate(validLinks):
                if self.isBlock(link, "è§£æå°ç»„å¸–å­åˆ—è¡¨"):
                    continue
                title = titles[index].strip()
                self.logger.info(f"[title|link]: å¯¹ä¸æ´»è·ƒå°ç»„çš„æœ‰æ•ˆå¸–å­è¿›è¡Œè¯·æ±‚: {title} > {link}")
                request = scrapy.Request(
                    link, headers=self.headers, callback=self.parse_detail_article
                )
                request.meta["tag_info"] = f"ä¸æ´»è·ƒå°ç»„çš„æœ€æ–°å¸–å­: {title}:{link}"
                request.meta["title"] = title
                self.markReqTitle(title, link)
                yield request

            return

        firstPageLink = l + "discussion?start=0"
        if self.isBlock(firstPageLink, f"å°ç»„: {t}") or self.isBlock(
            gid, f"å°ç»„: {gid} _ {t}"
        ):
            self.logger.info(f"æ˜¯å¦çˆ¬å–->è¯¥å°ç»„ {t} _ {gid} é“¾æ¥å·²Block: {firstPageLink}")
            return

        #  é¦–é¡µéƒ½æ˜¯ç¬¦åˆçš„æƒ…å†µä¸‹, å¾ªç¯è¯·æ±‚é¦–é¡µ
        if len(validLinks) > 40:
            request = scrapy.Request(
                l, headers=self.headers, dont_filter=True, callback=self.parseIfNeed
            )
            request.meta["tag_info"] = f" ç›‘æ§å°ç»„é¦–é¡µ: {t}:{l}"
            request.meta["title"] = t
            yield request

        # 1. è¿”å›åœ¨æŸå°ç»„é‡Œæ ¹æ®å…³é”®è¯æœç´¢çš„ç»“æœ(ç›´æ¥æ‹¼æ¥idå’Œå…³é”®è¯å³ä½¿æœç´¢ç»“æœçš„url)
        for key in parse_tool.getRegions():
            target_search_link = (
                "https://www.douban.com/group/search?group="
                + gid
                + "&cat="
                + "1013"
                + "&q="
                + key
                + "&sort=time"
            )
            if self.isBlock(target_search_link):
                continue
            # ä¿å­˜æœç´¢çš„é“¾æ¥
            monitorKey = f"{gid}_{key}"
            if monitorKey not in self.monitors:
                # self.logger.info(f'ğŸ‘€ ğŸ‘€ ğŸ‘€ ğŸ‘€ ğŸ‘€æ ‡è®°ç›‘æ§{monitorKey}, å°ç»„: {t}, å…³é”®å­—: {key}')
                self.monitors[monitorKey] = {
                    "for": f"goup:{t} -> {key}",
                    "link": target_search_link,
                    "monitor": False,
                }
            if self.isBlock(monitorKey, target_search_link):
                continue
            request = scrapy.Request(
                target_search_link,
                headers=self.headers,
                callback=self.parse_search_result,
            )
            request.meta["monitorKey"] = monitorKey
            request.meta["tag_info"] = f"ç»„: {t}, å…³é”®å­—æœç´¢: {key}"
            yield request

        #  ä¿å­˜æŸä¸ªç»„çš„é¦–é¡µ
        monitorKey = f"{gid}"
        if monitorKey not in self.monitors:
            # self.logger.info(f'ğŸ‘€ğŸ‘€ğŸ‘€ğŸ‘€ğŸ‘€æ ‡è®°ç›‘æ§{monitorKey}')
            self.monitors[monitorKey] = {
                "for": f"all group post: {t}",
                "link": firstPageLink,
                "monitor": False,
            }
        # 2. è¿”å›è§£ææŸä¸ªå°ç»„çš„å¸–å­çš„åˆ—è¡¨, å¯ä»¥ç¿»é¡µ
        if self.isBlock(l, "åˆ¤æ–­æ˜¯å¦éœ€è¦çˆ¬å–"):
            return
        gRequest = scrapy.Request(
            firstPageLink, headers=self.headers, callback=self.parse_group
        )
        gRequest.meta["monitorKey"] = monitorKey
        gRequest.meta["tag_info"] = f"å°ç»„å†…çˆ¬å–, ç»„{t}"
        yield gRequest

    def randomString(self, prefix="&", k=6):
        return f'{prefix}nonce={"".join(random.choices(string.ascii_letters + string.digits, k=k))}'

    def parse(self, response):
        self.logger.info("è§£æå°ç»„åˆ—è¡¨é¡µ")
        # å…ˆè¿›å°ç»„, çœ‹çœ‹éœ€ä¸éœ€è¦è¿›ä¸€æ­¥æœç´¢æˆ–è€…ç¿»é¡µ
        forceParse = response.meta.get("forceParse", False)
        if "stop" in self.configs and self.configs["stop"] and not forceParse:
            self.logger.info("ç»„å·²ç»èµ°è¿‡ä¸€æ¬¡, ä¸éœ€è¦åœ¨ç¿»é¡µè·å–")
            self.logger.info(
                f"ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«å·²ç»æˆåŠŸè·å–æ™šæ‰€æœ‰æœç´¢å‡ºçš„å°ç»„, å…±{len(self.groups.keys())}ä¸ªå°ç»„ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«"
            )
            for gid, dic in self.groups.items():
                link = dic.get("link")
                if link and self.isBlock(link, "è§£æå°ç»„åˆ—è¡¨"):
                    continue
                if dic.get("dead", False):
                    self.logger.info(f"ğŸš¯ğŸš¯ğŸš¯ğŸš¯ğŸš¯ğŸš¯è¯¥é“¾æ¥å·² dead: {link}")
                    continue
                request = scrapy.Request(
                    link + "discussion?start=50" + self.randomString(),
                    headers=self.headers,
                    callback=self.parseIfNeed,
                )
                request.meta["title"] = dic.get("title")
                request.meta["link"] = link
                request.meta["gid"] = gid
                yield request
            return

        groups = response.xpath('//div[@class="content"]')
        groupNum = len(groups)
        for idx, group in enumerate(groups):
            # å°ç»„æ ‡é¢˜
            # t = group.css('::text').extract_first().strip()
            t = group.xpath('.//div[@class="title"]/h3/a/text()').get().strip()
            if len(re.findall(r"åŒå¿—|æ‹‰æ‹‰", t)) > 0:
                continue
            # å°ç»„é“¾æ¥
            l = group.css("a::attr(href)").extract_first().strip()
            if self.isBlock(l):
                continue

            gid = group.css("a::attr(onclick)").re(".*sid: (\d+)}.*").pop()
            if gid in self.monitors and self.monitors[gid]["monitor"]:
                continue
            if gid in self.groups and self.groups[gid].get("dead", False):
                continue
            # æˆå‘˜æ•°
            # string(//div[@class="info"])
            request = scrapy.Request(
                l + "discussion?start=50" + self.randomString(),
                headers=self.headers,
                callback=self.parseIfNeed,
            )

            request.meta["title"] = t
            request.meta["link"] = l
            request.meta["gid"] = gid
            self.groups[gid] = dict(title=t, link=l, dead=False)
            yield request

        # 3. ä¸‹ä¸€é¡µ
        next = ""
        if "gidPage" in self.configs and self.start:
            next = self.configs["gidPage"]
        else:
            next = parse_tool.get_next_page_link(response)

        self.start = False
        if next and "http" in next:
            # start = re.findall(r'start=(\d+)&', next)
            self.logger.info(f"æœç»„ç»“æœ_ä¸‹ä¸€é¡µ: {next}")
            self.configs["gidPage"] = next
            if self.isBlock(next):
                return
            req = scrapy.Request(next, headers=self.headers, callback=self.parse)
            req.meta["forceParse"] = True
            yield req
        else:
            self.logger.info(f"!!!!!æœç´¢ç»„_å·²ç»æ‹¿ä¸åˆ°ä¸‹ä¸€é¡µ")
            if "gidPage" in self.configs:
                del self.configs["gidPage"]
            self.configs["stop"] = True

    def routineMonitor():
        # todo:  æŸä¸ªå°ç»„çš„å¸–å­æœå®Œä¹‹å, å°±å›åˆ°ç¬¬ä¸€é¡µå¼€å§‹ç›‘æ§.
        # è®°å½•æ¯ä¸ªå°ç»„çš„ç¬¬ä¸€é¡µ, æ¯ä¸ªå°ç»„æœç´¢çš„ç¬¬ä¸€é¡µ,
        pass

    def parse_group(self, response):
        # è§£ææŸå°ç»„çš„å¸–å­åˆ—è¡¨, ç”¨æ¥å…¨ç»„ä¾¿åˆ©ç¬¦åˆtitleæ¡ä»¶çš„å¸–å­

        tag_info = response.meta.get("tag_info", "no")
        monitorKey = response.meta.get("monitorKey")
        needMonitor = self.monitors.get(monitorKey, {}).get(
            "monitor", False
        )  # [monitorKey]['monitor']
        page = response.meta.get("page", 1)

        # å°ç»„æ ‡é¢˜
        gtl = response.css("head title::text").extract_first().strip()
        need_more = True
        # å¸–å­æ ‡é¢˜
        titles = response.css(".olt .title")
        totalTitleNum = len(titles)
        if (totalTitleNum == 0) or (page == 1 and totalTitleNum < 24):
            self.logger.info(
                f"çˆ¬å–å°ç»„å†…å®¹, ç¬¬ {page} é¡µtitleæ•°: {totalTitleNum}, åŠ å…¥ blockğŸš«ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«"
            )
            self.blockLink.append(response.url)
            self.monitors[monitorKey]["monitor"] = False
            return

        if page == 1:
            if monitorKey and self.monitors[monitorKey]:
                self.monitors[monitorKey]["monitor"] = True
            request = scrapy.Request(
                response.url,
                dont_filter=True,
                headers=self.headers,
                callback=self.parse_group,
            )
            request.meta["tag_info"] = f"{tag_info}"
            yield request

        self.logger.info(f" ğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒğŸŒ çˆ¬å– {gtl} å°ç»„ ç¬¬ {page} é¡µå†…å®¹, {totalTitleNum} æ¡å¸–å­")
        for idx, t in enumerate(titles):
            # è·å–æ—¶é—´
            title = t.css("a::attr(title)").extract_first().strip()
            link = t.css("a::attr(href)").extract_first()
            time = t.xpath('string(..//td[@class="time"][1])').extract_first().strip()
            if not parse_tool.is_need_parse(time, 1 if needMonitor else duetime):
                # æ—¶é—´ä¸ç¬¦åˆçš„è·³è¿‡
                need_more = False
                if page == 1 and idx < 10:
                    self.logger.info(
                        colorTitle(
                            f"é¡µé¢ç¬¦åˆè¦æ±‚çš„å¾ˆå°‘: ç›´æ¥è·³è¿‡: ç»„: {gtl}, page: {page}, index: {idx} > {time}>\næ ‡é¢˜: {t}, link: {link} | {time}"
                        )
                    )
                    return
                self.logger.info(
                    f"ğŸ•˜ğŸ•˜ğŸ•˜ğŸ•˜ğŸ•˜ğŸ•˜å°ç»„:{gtl}_æ—¶é—´è¿‡æœŸ: {time}, \næ ‡é¢˜: {title}, é“¾æ¥: {link} | {time}"
                )
                break
            # æ—¶é—´ç¬¦åˆè¦æ±‚
            #  æ¥¼ä¸»è¢«å±è”½çš„è·³è¿‡
            author = t.xpath("string(following-sibling::td[1])").extract_first().strip()
            if author in self.authors:  # ä»…ä»…åˆ¤æ–­åœ¨ä¸åœ¨, åœ¨çš„è¯å°±ä¸ç»§ç»­è¯·æ±‚è§£æè¿™ä¸ªå¸–å­äº†
                continue
            #  å¸–å­ title é‡å¤çš„è·³è¿‡
            if self.isCrawledTitle(title, link):
                continue
            # é»˜è®¤è§£æè¯¦æƒ…
            to_parse = self.parse_detail_article
            (isTarget, keys) = parse_tool.filter_title(
                title.strip() + author, f"è§£æå°ç»„: {title}", link
            )
            if isTarget:
                # titleä¸­å«æœ‰å…³é”®å­—çš„è¯å°±ç›´æ¥è§£æç›®çš„æ–‡ç« 
                to_parse = self.parse_target_article
            elif keys:
                continue
            if self.isBlock(link, "è§£æå°ç»„å¸–å­åˆ—è¡¨"):
                continue
            request = scrapy.Request(link, headers=self.headers, callback=to_parse)
            request.meta[
                "tag_info"
            ] = f"{tag_info}, å¸–å­: {idx}/{totalTitleNum} -> {colorTitle(title)}"
            request.meta["title"] = title
            self.markReqTitle(title, link)
            yield request

        # è·å–ä¸‹ä¸€é¡µ
        # self.showCurrentInfo()
        next = parse_tool.get_next_page_link(response)
        if next and need_more and not needMonitor:
            if self.isBlock(next, f"å°ç»„:{gtl}"):
                return
            req = scrapy.Request(next, headers=self.headers, callback=self.parse_group)
            req.meta["monitorKey"] = monitorKey
            req.meta["tag_info"] = tag_info
            req.meta["page"] = page + 1
            yield req
        else:
            self.logger.info(
                f"å°ç»„é¡µ_æ‹¿ä¸åˆ°ä¸‹ä¸€é¡µæˆ–ä¸éœ€è¦æ›´å¤š: {next} / needMore: {need_more}, æ˜¯å¦ç›‘æ§: {needMonitor}"
            )
            if monitorKey and self.monitors[monitorKey]:
                self.monitors[monitorKey]["monitor"] = True

            link = self.monitors[monitorKey]["link"]
            if self.isBlock(link, f"å°ç»„:{gtl}"):
                return
            monitorLink = link + self.randomString("?")
            req = scrapy.Request(
                monitorLink,
                dont_filter=True,
                headers=self.headers,
                callback=self.parse_group,
            )
            req.meta["monitorKey"] = monitorKey
            if "ç›‘æ§ä¸­" not in req.meta.get("tag_info", ""):
                req.meta["tag_info"] = tag_info + "_ç›‘æ§ä¸­ğŸ‘ğŸ‘ğŸ‘"
            self.logger.info(f"ğŸ‘{monitorKey}  __æ­£åœ¨ç›‘æ§: {monitorLink}")
            yield req

    def showCurrentInfo(self):
        return
        monitors = self.monitors
        self.logger.info(
            f"""
        ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–

        å½“å‰ç›‘æ§: {len(list(filter(lambda g: g["monitor"] and g["monitor"] == True, self.monitors)))} ä¸ª
        æœ‰æ•ˆå°ç»„: {len(list(filter(lambda g: g["dead"] and g["dead"] == False, self.groups)))} ä¸ª
        å±è”½æ•°é‡: {len(self.blockLink)}

        ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–
        """
        )

    def parse_search_result(self, response):
        self.logger.info(f"è§£ææ ¹æ®è®¾å®šçš„å…³é”®è¯çš„æœç´¢æŸå°ç»„çš„ç»“æœçš„å¸–å­\n{response.url}")

        tag_info = response.meta.get("tag_info", "no")
        monitorKey = response.meta.get("monitorKey")
        needMonitor = self.monitors.get(monitorKey, {}).get(
            "monitor", False
        )  # [monitorKey]['monitor']
        page = response.meta.get("page", 1)

        ci = response.css("title::text").re(r".*:(.+)$")
        # if len(ci) > 0:
        #     self.logger.info('æœç´¢çš„å…³é”®è¯: ' + ci[0] + '\n')

        gtl = response.css("head title::text").extract_first().strip()
        location = response.xpath("string(//h1/div)").get().strip()
        need_more = True
        titles = response.css("tbody .pl")
        self.logger.info(
            f"[ç›‘æ§]: ({needMonitor})_ç»„ä½ç½®: {tag_info} é¡µé¢->{len(titles)}æ¡title\n\n"
            + "^" * 100
        )
        totalTitleNum = len(titles)
        if (totalTitleNum == 0) or (page == 1 and totalTitleNum < 47):
            # åŠ å…¥é»‘åå•, è¿™ä¸ªä»¥åå°±ç›´æ¥è¿‡æ»¤æ‰
            self.logger.info(
                "æ¡ä»¶: (totalTitleNum == 0) or (page == 1 and totalTitleNum < 47)"
            )
            self.logger.info(
                f"çˆ¬å–æœç´¢ç»“æœ, ç¬¬ {page} é¡µtitleæ•°: {totalTitleNum}, åŠ å…¥ blockğŸš«ğŸš«ğŸš«ğŸš«ğŸš«ğŸš«\n{response.url}"
            )
            self.blockLink.append(response.url)
            self.blockLink.append(monitorKey)
            self.monitors[monitorKey]["monitor"] = False
            return

        # self.showCurrentInfo()
        for idx, t in enumerate(titles):
            title = t.css(".td-subject a::attr(title)").extract_first().strip()
            link = t.css(".td-subject a::attr(href)").extract_first().strip()
            time = t.css(".td-time::attr(title)").extract_first().strip()
            if not parse_tool.is_need_parse(time, 1 if needMonitor else duetime):
                need_more = False
                break
            if self.isCrawledTitle(title, link):
                continue
            if self.isBlock(link):
                continue

            # é»˜è®¤è§£æè¯¦æƒ…
            to_parse = self.parse_detail_article
            (isTarget, keys) = parse_tool.filter_title(
                title, f" è§£ææœç´¢å…³é”®è¯: {title}", link
            )
            if isTarget:
                # titleä¸­å«æœ‰å…³é”®å­—çš„è¯å°±ç›´æ¥è§£æç›®çš„æ–‡ç« 
                to_parse = self.parse_target_article
            elif len(keys) > 0:
                continue

            date = datetime.datetime.today().strftime("%Y-%m-%d %H:%M")
            self.logger.info(
                f"\nå¯ä»¥è§£æå¸–å­->æ—¶é—´: {date} \næ ‡é¢˜: {colorTitle(title)} | link: {link} | {date}"
            )
            # slackMe(' \n '.join([
            #     f'```{">"*15}{time} \nå¸–å­\nfrom: {tag_info} \n ` <{link}|{title}> `  \n{time} ```',
            #     f' ` <{link}|{title}> `  \n{time}'
            # ]))
            request = scrapy.Request(link, headers=self.headers, callback=to_parse)
            request.meta[
                "tag_info"
            ] = f"{tag_info}, å¸–å­: {idx}/{totalTitleNum} -> {title}"
            request.meta["title"] = title
            request.meta["monitorKey"] = monitorKey
            self.markReqTitle(title, link)
            yield request

        # è·å–ä¸‹ä¸€é¡µ
        next = parse_tool.get_next_page_link(response)
        if next and need_more and not needMonitor:
            if self.isBlock(next):
                return
            req = scrapy.Request(
                next, headers=self.headers, callback=self.parse_search_result
            )
            req.meta["tag_info"] = tag_info
            req.meta["monitorKey"] = monitorKey
            req.meta["page"] = page + 1
            self.logger.info(f"æœç´¢å…³é”®å­—ç¿»é¡µåˆ°: {page+1},  link: {next}")
            yield req
        else:
            self.logger.info(
                f"å°ç»„æŸ¥è¯¢å…³é”®å­—_æ‹¿ä¸åˆ°ä¸‹ä¸€é¡µæˆ–ä¸éœ€è¦æ›´å¤š: {next} / needMore: {need_more},  æ˜¯å¦ç›‘æ§: {needMonitor}"
            )
            if monitorKey and self.monitors[monitorKey]:
                self.monitors[monitorKey]["monitor"] = True
            link = self.monitors[monitorKey]["link"]
            if self.isBlock(link):
                return
            monitorLink = link + self.randomString()
            req = scrapy.Request(
                monitorLink,
                dont_filter=True,
                headers=self.headers,
                callback=self.parse_search_result,
            )
            self.logger.info(f" ğŸ‘ğŸ‘ {monitorKey}  __æ­£åœ¨ç›‘æ§: {monitorLink}")
            req.meta["monitorKey"] = monitorKey
            req.meta["tag_info"] = (
                f"{tag_info}_ç›‘æ§ä¸­ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘" if "ç›‘æ§" not in tag_info else tag_info
            )
            yield req

    def parse_detail_article(self, response):
        # è§£æå…·ä½“çš„å¸–å­, æŸ¥çœ‹æ˜¯å¦å«æœ‰éœ€è¦è§£æçš„å…³é”®è¦ç´ 
        reqTitle = response.meta.get("title")
        if reqTitle:
            self.markRespOfTitle(reqTitle, response.url)

        # è·å–æ¥¼ä¸»
        author = None
        if (
            response.css(".from a::text")
            and response.css(".from a::text").extract_first()
        ):
            author = response.css(".from a::text").extract_first().strip()

        completeTitle = response.css(".tablecc::text").extract_first()
        title = (completeTitle and completeTitle.strip()) or response.css(
            "#content h1::text"
        ).extract_first().strip()

        # è·å–æ­£æ–‡å†…å®¹
        content = (
            response.xpath('string(//div[@class="topic-richtext"])')
            .extract_first()
            .strip()
        )

        self.logger.info(f"è§£æå…·ä½“å¸–å­: {colorTitle(title)} | {response.url}")
        # åˆ¤æ–­æ˜¯å¦ç¬¦åˆç¬¬ä¸€éœ€è¦
        (isTarget, keys) = parse_tool.filter_title(
            content + author, title=f"è§£æå…·ä½“: {title}", link=response.url, use="content"
        )

        tagInfo = response.meta.get("tag_info", "no_tag_info")
        if isTarget:
            response.meta["key"] = keys
            # response.meta['content'] = content
            # response.meta['author'] = author
            self.parse_target_article(response)
            self.logger.info(
                f'\n\nè§£æç¬¦åˆè¦æ±‚çš„å¸–å­: tagInfo: \n{tagInfo} < {response.url} >\n\n{"ğŸ‰‘"*100}\n\n'
            )
            return
        else:
            self.logger.info(f"è§£ææ²¡æœ‰å‘½ä¸­è¦æ±‚çš„å¸–å­,  å‘½ä¸­: {isTarget}, å…³é”®è¯: {keys}")
            if len(keys) > 0:
                return
            else:
                # æ²¡æœ‰å±è”½è¯, é€šçŸ¥ç®€è¦å†…å®¹
                time = (
                    response.xpath(
                        'string(//span[@class="from"]/following-sibling::span)'
                    )
                    .get()
                    .strip()
                )
                if parse_tool.is_need_parse(time):
                    return
                date = datetime.datetime.today().strftime("%Y-%m-%d %H:%M")
                self.logger.info(
                    f'{">"*15}\nä¸æ˜¯ç›®æ ‡ä½†æ˜¯æ²¡å‘½ä¸­å±è”½è¯çš„:{date} > æ¥¼ä¸»: {author} > from: {tagInfo} > {time} > \næ ‡é¢˜:{colorTitle(title)} | {response.url} | {time}\n'
                )
            #   time = response.xpath(
            #     'string(//span[@class="from"]/following-sibling::span)').get(
            #     ).strip()
            #   date = datetime.datetime.today().strftime('%Y-%m-%d %H:%M')
            #   slackMe(' \n '.join([
            #       f'```{">"*15}{date} \nå¸–å­\næ¥¼ä¸»: {author}\nfrom: {tagInfo} \n ` <{response.url}|{title}> `  \n{time} ```',
            #       f' ` <{response.url}|{title}> `  \n{time}'
            #   ]))

            return

    def parse_target_article(self, response):
        # è§£æå«æœ‰å…³é”®è¯çš„å¸–å­
        reqTitle = response.meta.get("title")
        if reqTitle:
            self.markRespOfTitle(reqTitle, response.url)

        tagInfo = response.meta.get("tag_info", "no_tag_info")
        self.logger.info(f"â—ï¸â—ï¸â—ï¸â—ï¸â—ï¸â—ï¸â—ï¸â—ï¸â—ï¸â—ï¸ \nè§£æç›®æ ‡å¸–å­: {tagInfo} -> {response.url}")
        # è·å–æ¥¼ä¸»
        allContent = response.xpath('string(//div[@class="article"])').get().strip()
        if len(re.findall(r"å·²[å‡ºç§Ÿ]", allContent)) > 0:
            self.logger.info("\n å·²ç§Ÿ.........ğŸ’¢.ğŸ’¢.ğŸ’¢.ğŸ’¢.ğŸ’¢.ğŸ’¢")
            return
        time = (
            response.xpath('string(//span[@class="from"]/following-sibling::span)')
            .get()
            .strip()
        )
        if not parse_tool.is_need_parse(time):
            self.logger.info(f"\næ—¶é—´è¿‡æœŸ: {time}")
            return
        author = None
        if (
            response.css(".from a::text")
            and response.css(".from a::text").extract_first()
        ):
            author = response.css(".from a::text").extract_first().strip()
        if self.isCrawledAuthor(author):
            self.logger.info(f"è¯¥æ¥¼ä¸»å·²ç»å‘è¿‡è´´: {author}......â“.â“.â“.â“")
            return
        # content = response.xpath('//div[@class="topic-doc"]').get().strip()
        content = response.xpath('//div[@class="topic-doc"]')
        if content:
            content = content.get()
            if content:
                content = content.strip()

        # è½¬æ¢å›¾ç‰‡
        content = re.sub(r'<img src="(.*?).webp" .*?>', r" \nImage: \1  ", content, 4)
        content = scrapy.Selector(text=content).xpath(r"string(.)").get().strip()

        # è½¬æ¢ç”µè¯
        content = re.sub(r"(1[3578]\d{9})", r" <tel:\1|\1> ", content)

        # æ ‡æœºå¯èƒ½çš„ä»·æ ¼
        content = re.sub(r"([^\d/p])(\d{4})([\D]{1})", r"\1 `\2` \3", content)

        # å»å¤šä½™çš„ç©ºæ ¼
        content = re.sub(r" {2,}", " ", content)

        completeTitle = response.css(".tablecc::text").extract_first()
        title = (completeTitle and completeTitle.strip()) or response.css(
            "#content h1::text"
        ).extract_first().strip()
        replies = self.getReply(response, author)
        reply = "\n- ".join(replies)
        if len(re.findall(r"è±†å‹\d{6,9}", reply)) > 10:
            self.logger.info(f'å¯èƒ½æ˜¯æœºå™¨äººåˆ·å¸–: "è±†å‹ xxxx å›å¤æ•°é‡: {len(replies)} æ¡')
            return
        (hit, keys) = parse_tool.filter_title(
            content + reply + f"è§£æç›®æ ‡: {title}" + author,
            title,
            response.url,
            use="content",
        )

        def strip(s):
            return s.strip()

        if hit:
            # å…ˆé«˜äº®ä¸‹`å¥³`å…³é”®è¯, æœ‰äº›æ¼æ‰çš„, èµ·ç åœ¨çœ‹çš„æ—¶å€™å®¹æ˜“çœ‹å‡ºæ¥
            content = re.sub(r"(å¥³)", r" `\1` ", content)
            seperator = ">" * 15
            date = datetime.datetime.today().strftime("%Y-%m-%d %H:%M")
            content = re.sub(
                f"({parse_tool.appendKeys(parse_tool.getRegions())})",
                r" `\1` ",
                content,
            )
            fromRequest = tagInfo.split("->")[0]
            slackMe(
                " \n ".join(
                    [
                        f"```{seperator}{date} \næ¥¼ä¸»: {author}\nfrom: {fromRequest} \nå…³é”®è¯: `{list(map(strip, keys))}` ```",
                        f" ` <{response.url}|{title}> `  \n{time}",
                        content,
                        reply,
                    ]
                )
            )
            self.logger.info(
                f'\n{"âœ…"*10}\nå‘é€: {colorTitle(title)} \né“¾æ¥: {response.url}\n'
            )

        # å°†å»é‡æ•°æ®ä¿å­˜åˆ°æœ¬åœ°
        self.saveCrawled()

    def getReply(self, response, author):
        allReply = response.xpath('//div[@class="reply-doc content"]')
        replyContents = {}
        for idx, reply in enumerate(allReply):
            au = reply.xpath("string(./div[1]//a)").get().strip().replace(" ", "")
            t = reply.xpath("string(./div[1]//span)").get().strip()
            c = (
                reply.xpath('string(./p[@class=" reply-content"])')
                .get()
                .strip()
                .replace(" ", "")
            )
            c = re.sub(r"(1[3578]\d{9})", r" <tel:\1|\1> ", c)
            c = re.sub(r"([^\d/p])(\d{4})([\D]{1})", r"\1 `\2` \3", c)
            showAu = "æ¥¼ä¸»" if author == au else au

            replyContents[c] = dict(au=showAu, time=t, index=idx)
            # self.logger.info(f'[ {c} ]: {t} > {showAu}')
        ans = []
        for key, value in replyContents.items():
            ans.append(
                f'{value.get("index")}. [ {key} ]: {value.get("time")} > {value.get("au")}'
            )
        return ans

    def isBlock(self, link, info=None, url=None):
        if link in self.blockLink:
            #  ä¸€èˆ¬ä¸ä¼šå†æœ‰ä»€ä¹ˆå¸–å­åœ¨é‚£é‡Œå‘äº†, ç›´æ¥å›æ‹’
            self.logger.info(
                f"ğŸš¯ğŸš¯æ­»é“¾æˆ–è€…æ˜¯ä¸Šæ¬¡æ²¡æœ‰å†…å®¹çš„ç»“æœ, é“¾æ¥: {link}, \né¢å¤–ä¿¡æ¯: {info}, link: {url}"
            )
            return True
        else:
            return False

    # æ˜¯å¦æŠ“å»è¿‡æ ‡é¢˜
    def dealTitle(self, title):
        title = re.sub(" ", "", title)
        return title.strip().lower()[:30]

    def isCrawledTitle(self, title, link):
        oTitle = title
        title = self.dealTitle(title)
        if title in self.titles:
            # self.logger.info(f"title å»é‡: {title}")
            return True
        else:
            # æ ‡è®°åœ¨è¯·æ±‚, æœªå“åº”.
            # self.logger.info(f'æ ‡è®° title çˆ¬å–: {title}, link: {link}')
            self.titles[title] = dict(
                link=link, title=oTitle, hadResp=False, hadReq=False
            )
            return False

    #  æ ‡æœºå·²çˆ¬å–æœ‰è¿”å›, (åŒºåˆ«åˆ¤æ–­, ä»¥å…è¯·æ±‚åœ¨æ’é˜Ÿæ—¶å€™ä¸­æ–­äº†ä¹‹å, åœ¨å¯åŠ¨æ—¶)
    def markRespOfTitle(self, title, link):
        oTitle = title
        title = self.dealTitle(title)
        self.titles[title] = dict(link=link, title=oTitle, hadReq=True, hadResp=True)

    # æ ‡æœºå·²è¯·æ±‚, æœªè¿”å›
    def markReqTitle(self, title, link):
        oTitle = title
        title = self.dealTitle(title)
        self.titles[title] = dict(link=link, title=oTitle, hadReq=True, hadResp=False)

    # æ˜¯å¦æŠ“å»è¿‡ä½œè€…
    def isCrawledAuthor(self, author):
        if author in self.authors:
            return True
        else:
            self.authors.append(author)
            return False

    # ä¿å­˜æŠ“å–æ•°æ®
    def saveCrawled(self):
        if len(self.titles.keys()) > 0:
            with open("./titles.json", "w") as f:
                json.dump(self.titles, f)
                self.logger.info(f"ä¿å­˜ titles: {len(self.titles.keys())} æ¡")

        if len(self.authors) > 0:
            with open("./authors.json", "w") as f:
                json.dump(self.authors, f)
                self.logger.info(f"ä¿å­˜ authors: {len(self.authors)} æ¡")

        if len(self.groups.keys()) > 0:
            with open("./groups.json", "w") as f:
                json.dump(self.groups, f)
                self.logger.info(f"ä¿å­˜ groups: {len(self.groups.keys())} æ¡")

        if len(self.configs.keys()) > 0:
            with open("./configs.json", "w") as f:
                self.configs["block_link"] = self.blockLink
                self.configs["monitors"] = self.monitors
                json.dump(self.configs, f)
                self.logger.info(f"ä¿å­˜ block_link: {len(self.blockLink)} æ¡")
                self.logger.info(f"ä¿å­˜ monitor: {len(self.monitors.keys())} æ¡")

        if len(redirectCount.keys()) > 0:
            with open("./redirectCount.json", "w") as f:
                json.dump(redirectCount, f)
                self.logger.info(f"ä¿å­˜ ä»£ç†: {len(redirectCount.keys())} æ¡")

    # åŠ è½½æ•°æ®
    def loadCrawled(self):
        try:
            with open("./titles.json", "r") as f:
                try:
                    self.logger.info("åŠ è½½ titles.json")
                    data = json.load(f)
                    self.titles = data
                    self.logger.info(f"åŠ è½½{len(self.titles.keys())}æ¡æ ‡é¢˜")
                except Exception as error:
                    self.logger.info(f"titleæ–‡ä»¶é”™è¯¯{error}")
                    # self.titles = {}
                    raise
                # else:
                # self.titles = data
        except:
            self.logger.info("æ²¡æœ‰titleæ–‡ä»¶")
            self.titles = {}
        try:
            with open("./authors.json", "r") as f:
                try:
                    self.logger.info("åŠ è½½ authors.json")
                    data = json.load(f)
                    self.logger.info(f"åŠ è½½æ¥¼ä¸»: {len(data)}")
                    self.authors = data
                except:
                    self.logger.info("æ–‡ä»¶authors.jsonåŠ è½½å‡ºé”™")
                    raise
                # else:
                #     self.authors = data
        except:
            self.logger.info("æ²¡æœ‰authorsæ–‡ä»¶")
            self.authors = []
        try:
            with open("./groups.json", "r") as f:
                try:
                    self.logger.info("åŠ è½½ groups.json")
                    data = json.load(f)
                    self.logger.info(f"å°ç»„æ•°: {len(data)}")
                    self.groups = data
                except:
                    self.logger.info("æ–‡ä»¶groups.json åŠ è½½å‡ºé”™")
                    # self.groups = {}
                    raise
                # else:
                #     self.groups = data
        except:
            self.logger.info("æ²¡æœ‰groupsæ–‡ä»¶")
            self.groups = {}
        try:
            with open("./configs.json", "r") as f:
                try:
                    self.logger.info(" åŠ è½½ configs æ–‡ä»¶")
                    data = json.load(f)
                    block_link = data.get("block_link")
                    monitors = data.get("monitors")
                    self.configs = data
                    if "stop" not in data:
                        self.configs["stop"] = False
                    if "block_link" not in data:
                        self.configs["block_link"] = []
                    if "monitors" not in data:
                        self.configs["monitors"] = {}
                    self.blockLink = block_link if block_link else []
                    self.monitors = monitors if monitors else {}
                except:
                    self.logger.info("æ–‡ä»¶configs.json")
                    # self.configs = {}
                    raise
                # else:
                #     self.configs = data
                #     self.blockLink = blockLink if blockLink else []
                #     self.monitors = monitors if monitors else {}
        except:
            self.logger.info("æ²¡æœ‰configsæ–‡ä»¶")
            self.configs = {}
        try:
            with open("./redirectCount.json", "r") as f:
                try:
                    self.logger.info(" åŠ è½½ redirectCount æ–‡ä»¶")
                    data = json.load(f)
                    redirectCount = data
                except:
                    self.logger.info("æ–‡ä»¶redirectCount.json")
                    raise
        except:
            self.logger.info("æ²¡æœ‰redirectCountæ–‡ä»¶")
            redirectCount = {}
